---
title: "Update the tables of biosample and sra"
author: "Angel"
date: "2022-09-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

```{r}
# these are the "released " ones, there are ones that are not released, withdrawn or surpressed. 
# the newly downloaded sra and biosample table from NCBI since 2022-02-26 
newsra <- read_tsv('~/Downloads/new_sra_20220908.tsv')

newbiosample <- read_tsv('~/Downloads/new_biosample_20220908.tsv')

# the sra and biosample table up until 2022-02-26
sra <- read_csv('../data/save/all_sra_table_20220226.csv')
biosample <- read_csv('../data/save/all_biosample_table_20220226.csv')

allsample <- bind_rows(newbiosample, biosample)
allsra <- bind_rows(newsra, sra)

allsample %>% write_csv('../data/save/all_released_biosample_table_20220907.csv')
allsra %>% write_csv('../data/save/all_released_sra_table_20220907.csv')
```

```{r}
# all the 16s samples used in the nutrition project
meta <- read_csv('../../food_tree/data/cleaned_stool/all_samples_meta_p2d_fg9_updated.csv')

length(intersect(meta$sampleid, allsample$BioSample.name))

# the biosamples that need to be created
toadd <- setdiff(meta$sampleid, allsample$BioSample.name)

# create a table that has the sampleid, day relative to transplant, and a patient ID
todo <- meta %>% 
  filter(sampleid %in% toadd) %>% 
  select(sampleid, sdrt) %>% 
  mutate(pid = str_replace(sampleid, '\\w$',''))

# now go to NCBI and create new bioproject      
# continue by creating new biosamples
# prepare a table that you can upload to the attributes page
att <- tibble(
  Sample_name = todo$sampleid,
  Organism = 'human gut metagenome',
  host = 'Homo sapiens',
  isolation_source = 'stool',
  collection_date = 'not applicable',
  geo_loc_name = 'USA: New York',
  lat_lon = '40.7641 N 73.9568 W',
  patient_id = todo$pid,
  DayRelativeToNearestHCT = todo$sdrt
) 
dir.create('../data/nutrition')
att %>% 
  write_tsv('../data/nutrition/biosample_att.tsv')

# the biosamples are done created .
```

```{r}
# now continue creating new SRA submissions
# the biosample accession is needed to fill this form so download that from your submissions from NCBI
biosampleacc <- read_tsv('../data/nutrition/biosample_accession.tsv') %>% 
  select(sampleid = sample_name, accession)

# also find the file name from the asv_alpha diversity table
library(vdbR)
connect_database()
get_table_from_database('asv_alpha_diversity_ag')

files <- asv_alpha_diversity_ag %>% 
  filter(sampleid %in% todo$sampleid) %>% 
  mutate(R1 = str_glue('{oligos_id}_R1.fastq.gz'),
         R2 = str_glue('{oligos_id}_R2.fastq.gz')) %>% 
  mutate(p1 = str_glue('{path_pool}/{R1}'),
         p2 = str_glue('{path_pool}/{R2}'))

# join the above two tables together so the sampleids match
joined <- files %>% full_join(biosampleacc)

# now make a table that you can upload to the SRA attributes page
sra <- tibble(
  biosample_accession = joined$accession,
  library_ID = joined$sampleid,
  title = 'Stool sample of allo-HCT patient',
  library_strategy = 'amplicon',
  library_source = 'GENOMIC',
  library_selection = 'PCR',
  library_layout ='paired',
  platform= 'ILLUMINA',
  instrument_model= 'Illumina MiSeq',
  design_description='PCR amplification of 16S V4-V5 regions; Bead-beating, phenol chloroform DNA extraction',
  filetype= 'fastq',
  filename=joined$R1,
  filename2=joined$R2
)
sra  %>% 
  write_tsv('../data/nutrition/sra_att.tsv')
```

```{r}
# now need to download the files from cluster to local
# The path is already assembled in the previous lines
# I ususally create a bash script to download it
files %>% 
   select(p1, p2) %>% 
   gather() %>% 
   mutate(cmd = str_glue('scp daia1@lilac.mskcc.org:{value} .')) %>% 
   select(cmd) %>% 
   write_csv('../data/dl_27.sh', col_names = F)

# the 16s files are small in size you can just upload thru the browser if not too many samples

```

**These 27 entries can be downloaded from the biosample [to be released] section **
