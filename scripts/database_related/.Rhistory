#
library(tidyverse)
library(seqinr);
library(stringr); #For string replace.
#library(stringi); #For string replace.
#library(caroline); #for `dbWriteTable2`; #https://stackoverflow.com/questions/30276108/how-to-use-dbwritetable2-properly
library("biomformat");
library(tidyverse)
#library("xlsx"); #For read.xlsx function;
if(file.exists('~/projects/general/library/antoniostats/intervalcluster2.R')){
source('~/projects/general/library/antoniostats/intervalcluster2.R')
}else{
print("Intervalcluster2.R not loaded! Check path if you need it!")
}
source('/Users/daia1/pipeline/scripts/database_related/get_data_from_query_OTU.R')
source('/Users/daia1/pipeline/scripts/database_related/db_connect.R'); #Start a connection here; The connection variable is `con`.
getwd()
# temp for when I'm using Marissa's laptop
source('/Users/daia1/Downloads/MSK/MSS_pipeline-/scripts/database_related/get_data_from_query_OTU.R')
# if there is a sourcing problem of the above script then go to that script and run directly , the key is to
# have the get_data_from_query_otu function
source('/Users/daia1/Downloads/MSK/MSS_pipeline-/scripts/database_related/db_connect.R');
#warning("TO DO: ADD a step to get last `id` from database.")
my_dbWriteTable <- function(con, table_name, d_set_to_upload_reordered){
#I created this function to upload data to table and skip duplicates.
#I should add functionality to add `NULL` or `NA` values;
stop("THIS HAS TO BE UPDATED! I AM SEARCHING FOR A GOOD SOLUTION TO AVOID DUPLICATE CONFLIT. I tried `ON CONFLICT`` option, but it did not work");
q0p3_count_pre = get_data_from_query_OTU(0.3,table_name); #get count before upload;
q0p1 = get_data_from_query_OTU(0.1,table_name);
if( !all(q0p1$column_name==colnames(d_set_to_upload_reordered) ) ){
stop();
}
colnames_str = paste(colnames(d_set_to_upload_reordered),collapse = ",");
data_type_str = sprintf("(%s)", paste(q0p1$data_type,collapse = ",") );
data_type_str = gsub(pattern = "integer", "%d",data_type_str);
data_type_str = gsub(pattern = "text", "'%s'",data_type_str);
data_type_str = gsub(pattern = "date", "'%s'",data_type_str);
values = do.call("sprintf", c(data_type_str, d_set_to_upload_reordered));
#values = apply( d_set_to_upload_reordered[ , colnames(d_set_to_upload_reordered) ] , 1 , paste , collapse = " , " );
#values = sprintf("(%s)",values);
query= sprintf("insert into %s (%s) values %s", table_name, colnames_str,
paste(values, collapse = ","))
dbSendQuery(con, query);
q0p3_count_post = get_data_from_query_OTU(0.3, table_name); #get count after upload;
total_insert = q0p3_count_post$count - q0p3_count_pret$count;
print(sprintf("%d rows were inserted!", total_insert));
}
upload_data_from_query_OTU_check_and_submission <- function(table_name, d_set_to_upload ){
#Check if d_set is in proper format to be uploaded in `table_name`.
#Add uploaded_date to date_frame.
uploaded_date = format(Sys.time(),"%m-%d-%Y");
d_set_to_upload$uploaded_date = uploaded_date;
#Assign incremental key values starting on `maximum` value in current table.
q_key_max_cur = get_data_from_query_OTU(0.2,table_name);
if(is.na(q_key_max_cur$max)){
q_key_max_cur$max = 0;
}
d_set_to_upload$key = ( 1:length(d_set_to_upload$uploaded_date) ) + q_key_max_cur$max;
#Get columns for `column names`
q_column_names = get_data_from_query_OTU(0.1,table_name);
#stop("Add step to compare columns, re-order data, add key and upload!");
if( ! all(is.element(q_column_names$column_name, colnames(d_set_to_upload) )) ){
stop( sprintf("Some fields in table %s are not defined in upload dataset", table_name));
}
d_set_to_upload_reordered = d_set_to_upload[,q_column_names$column_name]
#Add the step to write the data into table!
print("data getting uploaded!");
dbWriteTable(con, table_name, value = d_set_to_upload_reordered, append = TRUE, row.names = FALSE);
#my_dbWriteTable(con, table_name, d_set_to_upload_reordered);
print( sprintf("upload: done! %d rows", length(d_set_to_upload_reordered$key)));
}
update_data_from_query_OTU_check_and_submission <- function(table_name, d_set_to_upload){
#This version check duplicates in table (according to table keys) and add only new, unique data.
#It intermediates this step with a tempory table.
#Some interesting discussion on skip duplicates: https://stackoverflow.com/questions/1009584/how-to-emulate-insert-ignore-and-on-duplicate-key-update-sql-merge-with-po
#Rationale:
#     1. Create temp_updating table with same structure as target table
#     2. Upload data to temporary table
#     3. Retrieve data in temporary table that is not in target table
#     4. Remove temporary table.
#     5. Return only novel data to be uploaded;
table_name = "metaphlan_shotgun_rel_abundance";
d_set_to_upload = d_set
table_name <- table_name
d_set_to_upload <- d_set
#Clean temp_table
temp_table="temp_updating";
query_check_temp = sprintf("SELECT EXISTS (SELECT 1 FROM   information_schema.tables WHERE table_name = '%s')",
temp_table);
q_check = dbGetQuery(con,query_check_temp);
if(q_check$exists){
dbSendQuery(con, sprintf("drop table %s", temp_table));
}
#Add uploaded_date to date_frame.
uploaded_date = format(Sys.time(),"%m-%d-%Y");
d_set_to_upload$upload_date = uploaded_date;#!!!!
#Assign incremental key values starting on `maximum` value in current table.
q_key_max_cur = get_data_from_query_OTU(0.2,table_name);
if(is.na(q_key_max_cur$max)){
q_key_max_cur$max = 0;
}
d_set_to_upload$key = ( 1:length(d_set_to_upload$upload_date) ) + q_key_max_cur$max;
#Get columns for `column names`
q_column_names = get_data_from_query_OTU(0.1,table_name);
#q_column_unique_names = get_data_from_query_OTU(0.1,sprintf("%s_unique",table_name));
#stop("Add step to compare columns, re-order data, add key and upload!");
if( ! all(is.element(q_column_names$column_name, colnames(d_set_to_upload) )) ){
stop( sprintf("Some fields in table %s are not defined in upload dataset", table_name));
}
d_set_to_upload_reordered = d_set_to_upload[,q_column_names$column_name]
print("    creating temporary table to check for duplicates");
query_create_temp_table = sprintf("CREATE TABLE %s AS SELECT * FROM %s WHERE 1=2",temp_table, table_name);
dbSendQuery(con,query_create_temp_table);
dbWriteTable(con, temp_table, value = d_set_to_upload_reordered, append = TRUE, row.names = FALSE);
#my_dbWriteTable(con, table_name, d_set_to_upload_reordered);
###getting data that does not violate unique constrain in target table.
query_select_unique_key = paste("select ccu.column_name, ccu.constraint_name from information_schema.table_constraints as tc",
" natural join information_schema.constraint_column_usage as ccu ",
#" on tc.table_name=ccu.table_name and tc.constraint_name=ccu.constraint_name ",
sprintf(" where tc.table_name='%s' and tc.constraint_type='UNIQUE'",table_name),
sep = "");
q_unique_key = dbGetQuery(con, query_select_unique_key);
if(length(unique(q_unique_key$constraint_name))>1){
stop("This table seems to have more than one rule for unique constraint. This script is not ready to work with it");
}
unique_constraint_str = paste(q_unique_key$column_name,collapse = ",");
#  query_select_novel_data = sprintf("select * from %s where %s as not in (select %s from %s)",
#                                    temp_table,
#                                    unique_constraint_str,
#                                    unique_constraint_str,
#                                    table_name);
unique_constraint_join_str = paste(sprintf("t1.%s=t2.%s",q_unique_key$column_name,q_unique_key$column_name), collapse = " AND ")
query_select_novel_data = paste(sprintf("select t1.* from %s as t1 left join %s as t2", temp_table, table_name),
sprintf(" on %s ", unique_constraint_join_str),
sprintf("where t2.%s is NULL", q_unique_key$column_name[1]),
collapse = " ");
d_set_unique = dbGetQuery(con,query_select_novel_data);
print(sprintf("%d/%d novel rows to be updated!",
dim(d_set_unique)[1],
dim(d_set_to_upload_reordered)[1]) );
if(length(d_set_unique)==0){
print("      No updates! No novel data.");
return();
}
upload_data_from_query_OTU_check_and_submission(table_name,d_set_unique);
}
upload_data_from_query_OTU <- function(query_number, ...){
args = list(...);
if(length(args)){
input_data_file = args[[1]];
}
if(query_number==1){
table_name = "shotgun_lookup_ad";
d_set_input = fread("~/Downloads/MSK/Catalog/data/new_20211014.csv")
d_set=data.frame(directory=d_set_input$directory,
projectid=d_set_input$projectid,
sampleid=d_set_input$sampleid,
fid=d_set_input$fid);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==2){
table_name = "picrust2_pathway_counts";
d_set_input = read_csv('~/pipeline/scripts/picrust2/data/normalized_picrust2_pred_pathway_abundance_all.csv')
d_set=data.frame(pwid=d_set_input$PWID,
sampleid=d_set_input$sampleid,
cpm=d_set_input$cpm);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==3){
table_name = "metacyc_pathway_name";
d_set_input = read_tsv('~/pipeline/scripts/shotgun_pipeline/data/metacyc_pathway_name_and_ID.tsv')
d_set=data.frame(pwid=d_set_input$PWID,
pw_name=d_set_input$pw_name);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==4){
table_name = "metacyc_pathway_ontology";
d_set_input = read_csv('~/pipeline/scripts/shotgun_pipeline/data/metacyc_pathway_class_and_superclass_levels.csv', col_types = 'ccccccccccccccccc')
d_set=data.frame(pwid=d_set_input$pwid,
l1=d_set_input$L1,
l2=d_set_input$L2,
l3=d_set_input$L3,
l4=d_set_input$L4,
l5=d_set_input$L5,
l6=d_set_input$L6,
l7=d_set_input$L7,
l8=d_set_input$L8,
l9=d_set_input$L9,
l10=d_set_input$L10,
l11=d_set_input$L11,
l12=d_set_input$L12,
l13=d_set_input$L13,
l14=d_set_input$L14,
l15=d_set_input$L15,
l16=d_set_input$L16);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==5){
table_name = "qpcr_16s_ag";
d_set_input = fread("~/Downloads/MSK/Catalog/data/qpcr_20211020.csv")
d_set=data.frame(sample_id=d_set_input$sample_id,
copy_number_16s=d_set_input$copy_number_16s,
copies_16s_per_g=d_set_input$copies_16s_per_g,
comments=d_set_input$comments,
sample_id_unique = d_set_input$sample_id_unique);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==6){
table_name = "samples_castori_ag";
input_data_file = '~/Desktop/tblSamples.csv'
Samples_castori_center_file = input_data_file;
castori_downloaded_date = '2022-04-29'
#castori_data = read.table(Samples_castori_center_file,sep=",", quote = "", comment.char = "", header = T);
castori_data = read_csv(Samples_castori_center_file)
mrn_integer = as.character(castori_data$MRN);
mrn_integer = suppressWarnings( as.numeric(mrn_integer) );
mrn_integer[is.na(mrn_integer)]=-1;
mrn_string = as.character(castori_data$MRN);
mrn_string[mrn_integer!=-1]=NA;
uploaded_date = format(Sys.time(),"%m-%d-%Y");
d_set = data.frame(#id = 1:length(castori_data$Sample_ID),
sampleid = castori_data$Sample_ID,
mrn = mrn_integer,
mrn_str = mrn_string,
datecollection = lubridate::mdy_hms(castori_data$DateCollection),
sampletype = castori_data$SampleType,
consistency = castori_data$Consistency,
datereceived = as.Date(castori_data$DateReceived,"%d-%b-%y"),
datealiquot = as.Date(castori_data$DateAliquot,"%d-%b-%y"),
boxrawstool = castori_data$BoxRawStool,
numberaliquots = castori_data$NumberAliquots,
numberstartingaliquots = castori_data$NumberStartingAliquots,
comment = castori_data$Comment,
timecollected = castori_data$TimeCollected,
dropofftime = castori_data$DropOffTime,
castori_downloaded_date =  lubridate::ymd(castori_downloaded_date),
#uploaded_date = uploaded_date,
stringsAsFactors = F);
no_date_str = as.Date("5/5/5555", "%m/%d/%Y");
col_names = colnames(d_set);
is_date_type = is.element(col_names, c( "DateCollection", "DateReceived", "DateAliquot", "castori_downloaded_date"));
ind_date_type = which(is_date_type);
d_set[,is_date_type][is.na(d_set[,is_date_type])] = no_date_str;
d_set[,ind_date_type]=format(d_set[,ind_date_type],"%m-%d-%Y")
#d_set[,ind_date_type]= as.character(format(d_set[,ind_date_type],"%m-%d-%Y"));
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
if(query_number==7){
table_name = "metaphlan_shotgun_rel_abundance";
d_set_input = read_csv("~/pipeline/scripts/shotgun_pipeline/data/metaphlan_cleaned_220502.csv")
d_set=data.frame(
full_id=d_set_input$full_id,
clade_name=d_set_input$clade_name,
clade_taxid=d_set_input$clade_taxid,
relative_abundance=d_set_input$relative_abundance,
coverage = d_set_input$coverage,
estimated_number_of_reads_from_the_clade = d_set_input$estimated_number_of_reads_from_the_clade,
CHOCOPhlAn_version = d_set_input$CHOCOPhlAn_version);
update_data_from_query_OTU_check_and_submission(table_name, d_set);
}
}
# 4-29-2022
# run the code starting from  table_name to d_set[,ind_date_type]=format(d_set[,ind_date_type],"%m-%d-%Y")
# and then run the code update_data_from_query_OTU_check_and_submission(table_name, d_set);
upload_data_from_query_OTU(7)
temp_table
d_set_to_upload_reordered = d_set_to_upload[,q_column_names$column_name]
print("    creating temporary table to check for duplicates");
query_create_temp_table = sprintf("CREATE TABLE %s AS SELECT * FROM %s WHERE 1=2",temp_table, table_name);
temp_table
table_name
query_create_temp_table
dbSendQuery(con,query_create_temp_table);
dbWriteTable(con, temp_table, value = d_set_to_upload_reordered, append = TRUE, row.names = FALSE);
query_select_unique_key = paste("select ccu.column_name, ccu.constraint_name from information_schema.table_constraints as tc",
" natural join information_schema.constraint_column_usage as ccu ",
#" on tc.table_name=ccu.table_name and tc.constraint_name=ccu.constraint_name ",
sprintf(" where tc.table_name='%s' and tc.constraint_type='UNIQUE'",table_name),
sep = "");
query_select_unique_key
q_unique_key = dbGetQuery(con, query_select_unique_key);
q_unique_key
if(length(unique(q_unique_key$constraint_name))>1){
stop("This table seems to have more than one rule for unique constraint. This script is not ready to work with it");
}
q_unique_key
View(d_set_to_upload_reordered)
unique_constraint_str = paste(q_unique_key$column_name,collapse = ",");
unique_constraint_str
#  query_select_novel_data = sprintf("select * from %s where %s as not in (select %s from %s)",
#                                    temp_table,
#                                    unique_constraint_str,
#                                    unique_constraint_str,
#                                    table_name);
unique_constraint_join_str = paste(sprintf("t1.%s=t2.%s",q_unique_key$column_name,q_unique_key$column_name), collapse = " AND ")
unique_constraint_join_str
#Get columns for `column names`
q_column_names = get_data_from_query_OTU(0.1,table_name);
q_column_names
#Jun/20/2017
#This script pre-defined a set of table. When possible, tables are named according to Xavier's database (`plvglover1`,  `microbiome`).
#The new table initiates empty.
#Every table are created with field: `key` for primary key and `uploaded_data` to keep track when data was uploaded.
#otu_ag table has the additional field `running_day` to keep track of when dataset was created.
#
#The default annotation for fields that refer to foreign keys is the `FOREIGNTABLENAME` + `_key` `FOREIGNTABLENAME_key`.
source('/Users/daia1/pipeline/scripts/database_related/db_connect.R'); #Initialize connection.
check_column_exists <- function(connection, table, column) {
query = paste( "SELECT column_name FROM information_schema.columns",
sprintf("WHERE table_name='%s' and column_name='%s'",
table,
column),
sep=" ");
r = dbGetQuery(con, query);
col_exists = length(r)>0;
return(col_exists);
}
add_field <- function(connection, target_table, new_field, field_type) {
#reference_table : table to be cloned.
query = paste( sprintf("alter table %s ", target_table),
sprintf("ADD column %s %s", new_field, field_type),
sep=" ");
col_exists = check_column_exists(con, target_table, new_field);
if(col_exists){
warning( sprintf("table %s already exists", target_table));
}else{
dbSendQuery(con, query);
}
}
grant_access <- function(connection, grant_type="restricted", table_name){
query_grant_read_access = sprintf("GRANT SELECT ON table %s TO littmane, taury, visitor, xavierj, gomesa, mooreg, peledj, guest",
table_name);
#When creating a new user, I need to grant `usage` to schema (see this: https://dba.stackexchange.com/questions/98892/minimal-grants-for-readonly-single-table-access-on-postgresql)
#e.g. grat usage on schema public to GUEST
dbSendQuery(con, query_grant_read_access);
if(grant_type=="opened"){
#query_grant_access = "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO littmane, taury, visitor, xavierj, gomesa";
query_grant_access = sprintf("GRANT SELECT, INSERT, UPDATE, DELETE ON table %s TO littmane, taury, visitor, xavierj, gomesa",
table_name);
}
if(grant_type=="restricted"){
query_grant_access = sprintf("GRANT SELECT, INSERT, UPDATE, DELETE ON table %s TO littmane, taury, xavierj, gomesa",
table_name);
}
dbSendQuery(con, query_grant_access);
}
add_unique <- function(connection, table_name, unique_set=NULL, not_null_constraint=T){
if(is.null(unique_set)){
return();
}
unique_str = sprintf("(%s)",
paste(unique_set, collapse = ","));
query_unique = sprintf("ALTER TABLE %s ADD CONSTRAINT %s_unique UNIQUE %s;",
table_name,
table_name,
unique_str);
dbSendQuery(con, query_unique);
if(not_null_constraint){
for(us in unique_set){
not_null_str = sprintf( "alter table %s alter %s set not null",
table_name,
us);
dbSendQuery(con, not_null_str);
}
}
}
create_table <- function(connection, table_name, table_fields, field_type, unique_set = NULL,
add_key_field=T, create_uploaded_date_field=T, access_type="restricted") {
#reference_table : table to be cloned.
#TO DO: Create primary key: `ALTER TABLE samples_temp_ag ADD PRIMARY KEY (id);`
already_exists = dbExistsTable(con, table_name);
if(add_key_field){
#This is how `primary key` is created: alter table patients_ag add column key serial primary key ;
if( all(table_fields!="key") ){
table_fields = c("key", table_fields);
field_type = c("serial primary key", field_type);
}
}
table_and_field_str = paste(sprintf("%s %s", table_fields, field_type), collapse = " , ");
query = paste( sprintf("create table %s ", table_name),
sprintf("( %s )", table_and_field_str),
sep=" ");
if(already_exists){
warning( sprintf("table %s already exists", table_name));
}
else{
dbSendQuery(connection, query);
}
if(create_uploaded_date_field){
new_field = "uploaded_date";
field_type = "date"
add_field(connection, table_name, new_field, field_type);
}
grant_access(connection, access_type, table_name);
#if(!is.null(unique_set)){
add_unique(connection, table_name, unique_set);
#}
# 1
return(table_name);
}
create_table_type <- function(query_type, ...){
#1. Samples: I believe it is better to have three distinct tables and join them with proper id.
#I should add a field `source` on counts table to describe where the sample_id cames from.
#1.1 samples_castori_ag;
#1.2 samples_duke_ag;
#1.3 samples_regensburg_ag;
if(query_type==1){
table_name = "shotgun_lookup_ad";
table_fields=c("directory","projectid","sampleid","fid");
field_type=c("text","text","text","text");
unique_set = c("projectid","sampleid");
create_table(con, table_name, table_fields, field_type, unique_set = unique_set,access_type="restricted");
}
if(query_type==2){
table_name = "picrust2_pathway_counts";
table_fields=c("pwid","sampleid","cpm");
field_type=c("text","text","real");
unique_set = c("pwid","sampleid");
create_table(con, table_name, table_fields, field_type, unique_set = unique_set,access_type="restricted");
}
if(query_type==3){
table_name = "metacyc_pathway_name";
table_fields=c("pwid","pw_name");
field_type=c("text","text");
unique_set = c("PWID");
create_table(con, table_name, table_fields, field_type, unique_set = unique_set,access_type="restricted");
}
if(query_type==4){
table_name = "metacyc_pathway_ontology";
table_fields=c("pwid","l1","l2","l3","l4",
"l5","l6","l7","l8","l9",
"l10","l11","l12","l13","l14",
"l15","l16");
field_type=c("text","text","text","text","text",
"text","text","text","text","text",
"text","text","text","text","text",
"text","text");
unique_set = c("pwid");
create_table(con, table_name, table_fields, field_type, unique_set = unique_set,access_type="restricted");
}
}
create_table_prepare <- function(d_set_input){
#This function prepares fields for creating a new data table.
field_name_str = gsub("\\.","_",tolower(colnames(d_set_input)));
n_fields = length(field_name_str);
cat("table_fields=c(");
n_loops = ceiling( (n_fields-1)/5);
for(i in 1:(n_loops)){
i_start=5*(i-1) +1;
i_end= min(5*(i), n_fields-1);
if(i==1){
cat("\"");
}else{
cat("\n\"");
}
cat(field_name_str[i_start:i_end],sep="\",\"");
cat("\",");
}
cat(sprintf("\"%s\");",field_name_str[n_fields]));
cat("\n");
#Printing field_type_str
field_type_str = rep("NA", n_fields);
for(i in 1:n_fields){
cur_class = class(d_set_input[[i]]);
if(cur_class=="factor"){
field_type_cur="text";
}
if(cur_class=="numeric" | cur_class=="integer"){
if(all(floor(d_set_input[[i]])==d_set_input[[i]]) &
!is.na(all(floor(d_set_input[[i]])==d_set_input[[i]]))){
field_type_cur="integer";
}
else{
field_type_cur="real";
}
}
if(cur_class=="Date"){
field_type_cur="date";
}
if(cur_class=="character"){
field_type_cur="text";
}
if(cur_class=="logical"){
field_type_cur="boolean";
}
field_type_str[i] = field_type_cur;
}
cat("\n")
cat("field_type=c(");
for(i in 1:n_loops){
i_start=5*(i-1) +1;
i_end= min(5*(i), n_fields-1);
if(i==1){
cat("\"");
}else{
cat("\n\"");
}
cat(field_type_str[i_start:i_end],sep="\",\"");
cat("\",");
}
cat(sprintf("\"%s\");",field_type_str[n_fields]));
cat("\n");
cat("\n");
#Printing d_set part in upload_data...
cat("d_set=data.frame(");
field_name_raw = colnames(d_set_input);
for(i in 1:(n_fields-1)){
cat(sprintf("%s=d_set_input$%s,\n",field_name_str[i],field_name_raw[i]))
}
cat(sprintf("%s=d_set_input$%s);",field_name_str[n_fields],field_name_raw[n_fields]))
}
View(create_table)
